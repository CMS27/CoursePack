{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Economics: Unconstrained Optimization\n",
    "\n",
    "\n",
    "Florian Oswald\n",
    "Sciences Po, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Taxonomy and Initial Examples\n",
    "\n",
    "* In most of the examples to follow, we talk about *minimization* of a function $f$. Everything we do also applies to maximization, since $\\min_x f(x) = \\max_x -f(x)$.\n",
    "* Here is a generic optimization problem:\n",
    "\t$$ \n",
    "        \\min_{x\\in\\mathbb{R}^n} f(x) \\text{ s.t. } \n",
    "        \\begin{array}{cc} c_i(x) = 0, & i \\in E \\\\\n",
    "        c_i(x) \\geq 0, & i\\in I \n",
    "        \\end{array}\n",
    "    $$\n",
    "\n",
    "* This is a general way of writing an optimization problem. E are all indices as equality constraints, I are all inequality constraints.\n",
    "* An example of such a problem might be\n",
    "\t$$ \n",
    "        \\min (x_1 -2)^2 + (x_2 -1)^2 \\text{ s.t. } \n",
    "        \\begin{array}{cc}\n",
    "         x_{1}^2 -x_2 \\leq 0 \\\\\n",
    "         x_1 +x_2 \\leq 2 \n",
    "         \\end{array}\n",
    "    $$\n",
    "* Here is a picture of that problem taken from the textbook [@nocedal-wright] ( for copyright reasons, I cannot show this in the online version of the slides. ):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 1.1 in [@nocedal-wright]](assets/figs-restricted/feasible-region.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kinds of problems considered\n",
    "\n",
    "* Don't talk about stochastic optimization methods:\n",
    "\t* Simluated Annealing\n",
    "\t* MCMC \n",
    "\t* other Stochastic Search Methods\n",
    "\t* A gentle introduction is \t[@casella-R]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transportation Problem\n",
    "\n",
    "> A chemical company has two factories $F_1,F_2$ and a dozen retail outlets $R_1,\\dots,R_{12}$. Each factory $i$ can produce at most $a_i$ tons of output each week. Each retail outlet $j$ has a weekly demand of $b_j$ tons per week. The cost of shipping from $F_i$ to $R_j$ is given by $c_{ij}$.\n",
    "> How much of the product to ship from each factory to each outlet, minimize cost, and satisfy all constraints? let's call $x_{ij}$ the number of tons shipped from $i$ to $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![Figure 1.2 in [@nocedal-wright]](assets/figs-restricted/transportation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A mathematical formulation of this problem is\n",
    "\t$$ \n",
    "        \\begin{aligned}\n",
    "\t\t\\min \\sum_{ij} c_{ij} x_{ij} \\\\\n",
    "\t\t\\text{subject to} \\sum_{j=1}^{12} x_{ij} \\leq a_i,\\quad i=1,2 \\\\\n",
    "\t\t\\sum_{i=1}^2  x_{ij} \\geq b_j,\\quad j=1,\\dots,12  \\\\\n",
    "\t\tx_{ij} \\geq 0, \\quad i=1,2,j=1,\\dots,12\n",
    "\t\t\\end{aligned}\n",
    "\t$$\n",
    "\n",
    "* This is called a *linear programming* problem, because both objective function and all constrains are linear.\n",
    "* With any of those being nonlinear, we would call this a non-linear problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constrained vs Unconstrained\n",
    "\n",
    "* There are many applications of both in economics.\n",
    "* Unconstrained: maximimum likelihood\n",
    "* Constrained: MPEC\n",
    "* It is sometimes possible to transform a constrained problem into an unconstrained one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convexity\n",
    "\n",
    "* Convex problems are easier to solve.\n",
    "* What is convex?\n",
    "\n",
    "> A set $S\\in\\mathbb{R}^n$ is convex if the straight line segment connecting any two points in $S$ lies entirely inside $S$.\n",
    "> A function $f$ is a convex function, if its domain $S$ is a convex set, and for any two points $x,y \\in S$, we have that\n",
    "\t$$ f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha) f(y) $$\n",
    "\tfor all $\\alpha \\in [0,1]$\n",
    "\n",
    "* Simple instances of convex sets are the unit ball $\\{y \\in \\mathbb{R}^n, \\Vert y\\Vert_2 \\leq 1\\}$, and any set defined by linear equalities and inequalities.\n",
    "* *convex Programming* describes a special case of the introductory minimizatin problem where \n",
    "\t* the objective function is convex, \n",
    "\t* the equality constrains are linear, and \n",
    "\t* the inequality constraints are concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "* All of the algorithms we are going to see employ some kind of *iterative* proceedure. \n",
    "* They try to improve the value of the objective function over successive steps.\n",
    "* The way the algorithm goes about generating the next step is what distinguishes algorithms from one another.\n",
    "\t* Some algos only use the objective function\n",
    "\t* Some use both objective and gradients\n",
    "\t* Some add the Hessian\n",
    "\t* and many variants more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Desirable Features of any Algorithm\n",
    "\n",
    "* Robustness: We want good performance on a wide variety of problems in their class, and starting from *all* reasonable starting points.\n",
    "* Efficiency: They should be fast and not use an excessive amount of memory.\n",
    "* Accuracy: They should identify the solution with high precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Word of Caution\n",
    "\n",
    "* You should **not** normally attempt to write a numerical optimizer for yourself.\n",
    "* Entire generations of Applied Mathematicians and other numerical pro's have worked on those topics before you, so you should use their work.\n",
    "\t* Any optimizer you could come up with is probably going to perform below par, and be highly likely to contain mistakes.\n",
    "\t* Don't reinvent the wheel.\n",
    "* That said, it's very important that we understand some basics about the main algorithms, because your task is **to choose from the wide array of available ones**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unconstrained Optimization: What is a solution?\n",
    "\n",
    "* A typical unconstrained optimization problem will look something like this:\n",
    "\t$$ \\min_x f(x),\\quad x\\in \\mathbb{R}^n $$\n",
    "\tand where $f : \\mathbb{R}^n \\mapsto \\mathbb{R}$ is a smooth function.\n",
    "* In general, we would always like to find a *global* minimizer, i.e. a point $$x^* \\text{ where } f(x^*) \\leq f(x)\\quad \\forall x$$\n",
    "* Since our algorithm is not going to visit many points in $\\mathbb{R}^n$ (or so we hope), we can never be totally sure that we find a global optimizer.\n",
    "* Most optimizers can only find a *local* minimizer. That is a point\n",
    " $$x^* \\text{ where } f(x^*) \\leq f(x)\\quad \\forall x \\in \\mathcal{N}$$ where $\\mathcal{N}$ is a neighborhood around $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Global minization can be very hard sometimes.\n",
    "\n",
    "![Global min at $f(512, 404.2319)$. By Gaortizg [GFDL](http://www.gnu.org/copyleft/fdl.html) or [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons](assets/figs/optimization/Eggholder-function.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Unconstrained) Optimization in `Julia`\n",
    "\n",
    "* Umbrella Organisation: [`http://www.juliaopt.org`](http://www.juliaopt.org)\n",
    "\t* We will make ample use of this when we talk about constrained optimsation.\n",
    "\t* The Julia Interface to the very well established [C-Library NLopt](http://ab-initio.mit.edu/wiki/index.php/NLopt) is called [`NLopt.jl`](https://github.com/JuliaOpt/NLopt.jl). One could use `NLopt` without constraints in an unconstrained problem.\n",
    "* [`Roots.jl`](https://github.com/JuliaLang/Roots.jl): Simple algorithms that find the zeros of a univariate function.\n",
    "* Baseline Collection of unconstrained optimization algorithms: [`Optim.jl`](https://github.com/JuliaOpt/Optim.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introducing [`Optim.jl`](https://github.com/JuliaOpt/Optim.jl)\n",
    "\n",
    "* Multipurpose unconstrained optimization package \n",
    "\t* provides 8 different algorithms with/without derivatives\n",
    "\t* univariate optimization without derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Golden Ratio or Bracketing Search for 1D problems\n",
    "\n",
    "* A derivative-free method\n",
    "* a Bracketing method\n",
    "\t* find the local minimum of $f$ on $[a,b]$\n",
    "\t* select 2 interior points $c,d$ such that $a<c<d<b$\n",
    "\t\t* $f(c) \\leq f(d) \\implies$ min must lie in $[a,d]$. replace $b$ with $d$, start again with $[a,d]$\n",
    "\t\t* $f(c) > f(d) \\implies$ min must lie in $[c,b]$. replace $a$ with $c$, start again with $[c,b]$\n",
    "\t* how to choose $b,d$ though?\n",
    "\t* we want the length of the interval to be independent of whether we replace upper or lower bound\n",
    "\t* we want to reuse the non-replaced point from the previous iteration. \n",
    "\t* these imply the golden rule:\n",
    "\t* new point $x_i = a + \\alpha_i (b-a)$, where $\\alpha_1 = \\frac{3-\\sqrt{5}}{2},\\alpha_2=\\frac{\\sqrt{5}-1}{2}$\n",
    "\t* $\\alpha_2$ is known as the *golden ratio*, well known for it's role in renaissance art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bracketing Search in Julia\n",
    "\n",
    "* The package [`Optim.jl`](https://github.com/JuliaNLSolvers/Optim.jl) provides an implementation of \"Brent's Method\" as well as the golden section search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brent = Results of Optimization Algorithm\n",
      " * Algorithm: Brent's Method\n",
      " * Search Interval: [0.000000, 2.000000]\n",
      " * Minimizer: 8.310315e-01\n",
      " * Minimum: -1.818739e+00\n",
      " * Iterations: 12\n",
      " * Convergence: max(|x - x_upper|, |x - x_lower|) <= 2*(1.5e-08*|x|+2.2e-16): true\n",
      " * Objective Function Calls: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition f(Any) in module Main at In[1]:2 overwritten at In[9]:3.\n",
      "WARNING: Method definition minf(Any) in module Main at In[2]:3 overwritten at In[9]:4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden = Results of Optimization Algorithm\n",
      " * Algorithm: Golden Section Search\n",
      " * Search Interval: [0.000000, 2.000000]\n",
      " * Minimizer: 8.310315e-01\n",
      " * Minimum: -1.818739e+00\n",
      " * Iterations: 37\n",
      " * Convergence: max(|x - x_upper|, |x - x_lower|) <= 2*(1.5e-08*|x|+2.2e-16): true\n",
      " * Objective Function Calls: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" />"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "using Optim\n",
    "f(x) = exp(x) - x^4\n",
    "minf(x) = -f(x)\n",
    "brent = optimize(minf,0,2,Brent())\n",
    "golden = optimize(minf,0,2,GoldenSection())\n",
    "println(\"brent = $brent\")\n",
    "println(\"golden = $golden\")\n",
    "plot(f,0,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid maximizer is 0.8247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition minf(Any) in module Main at In[1]:3 overwritten at In[2]:3.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: optimize not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: optimize not defined",
      ""
     ]
    }
   ],
   "source": [
    "# how well does this do with many local minima?\n",
    "fun(x) = exp(x) - x^4 +sin(40*x)\n",
    "minf(x) = -fun(x)\n",
    "grid = collect(0:0.0001:2);\n",
    "v,idx  = findmax(Float64[fun(x) for x in grid])\n",
    "println(\"grid maximizer is $(grid[idx])\")\n",
    "golden = optimize(minf,0,2,GoldenSection())\n",
    "brent = optimize(minf,0,2,Brent())\n",
    "using Base.Test\n",
    "println(\"brent minimizer = $(brent.minimizer)\")\n",
    "println(\"golden minimizer = $(golden.minimizer)\")\n",
    "plot(fun,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond One Dimension\n",
    "\n",
    "### Introducing [Rosenbrock's Banana](https://en.wikipedia.org/wiki/Rosenbrock_function) function\n",
    "\n",
    "The Banana function is defined by \n",
    "\t$$ f(x,y) = (a-x)^2  + b(y-x^2)^2  $$\n",
    "\n",
    "![Banana for $a = 0$. By Gaortizg [GFDL](http://www.gnu.org/copyleft/fdl.html) or [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons](assets/figs/optimization/rosenbrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is the minimum of that function?\n",
    "\n",
    "* For $a=1,b=100$, what is the global minimum of that function?\n",
    "* What are the inputs one needs to supply to an algorithm in a more general example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rosenbrock Banana and Optim.jl\n",
    "\n",
    "* We will use `Optim` for the rest of this lecture.\n",
    "* We need to supply the objective function and - depending on the solution algorithm - the gradient and hessian as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optim.UnconstrainedProblems.OptimizationProblem(\"Rosenbrock\",Optim.UnconstrainedProblems.rosenbrock,Optim.UnconstrainedProblems.rosenbrock_gradient!,Optim.UnconstrainedProblems.rosenbrock_hessian!,[0.0,0.0],[1.0,1.0],true,true)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "rosenbrock = Optim.UnconstrainedProblems.examples[\"Rosenbrock\"]\n",
    "\n",
    "# contains:\n",
    "# function rosenbrock(x::Vector)\n",
    "#     return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "# end\n",
    "\n",
    "# function rosenbrock_gradient!(x::Vector, storage::Vector)\n",
    "#     storage[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "#     storage[2] = 200.0 * (x[2] - x[1]^2)\n",
    "# end\n",
    "\n",
    "# function rosenbrock_hessian!(x::Vector, storage::Matrix)\n",
    "#     storage[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "#     storage[1, 2] = -400.0 * x[1]\n",
    "#     storage[2, 1] = -400.0 * x[1]\n",
    "#     storage[2, 2] = 200.0\n",
    "# end\n",
    "\n",
    "# there are many other examples on Optim.UnconstrainedProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparison Methods\n",
    "\n",
    "* We will now look at a first class of algorithms, which are very simple, but sometimes a good starting point.\n",
    "* They just *compare* function values.\n",
    "* *Grid Search* : Compute the objective function at $G=\\{x_1,\\dots,x_N\\}$ and pick the highest value of $f$. \n",
    "\t* This is very slow.\n",
    "\t* It requires large $N$.\n",
    "\t* But it's robust (will find global optimizer for large enough $N$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search results in minimizer = [1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "# grid search on rosenbrock\n",
    "grid = collect(-1.0:0.1:3);\n",
    "grid2D = [[i;j] for i in grid,j in grid];\n",
    "val2D = map(rosenbrock.f,grid2D);\n",
    "r = findmin(val2D);\n",
    "println(\"grid search results in minimizer = $(grid2D[r[2]])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bracketing for Multidimensional Problems: Nelder-Mead\n",
    "\n",
    "* The Goal here is to find the simplex containing the local minimizer $x^*$\n",
    "* In the case where $f$ is n-D, this simplex has $n+1$ vertices\n",
    "* In the case where $f$ is 2-D, this simplex has $2+1$ vertices, i.e. it's a triangle.\n",
    "* The method proceeds by evaluating the function at all $n+1$ vertices, and by replacing the worst function value with a new guess.\n",
    "* this can be achieved by a sequence of moves:\n",
    "\t* reflect\n",
    "\t* expand\n",
    "\t* contract\n",
    "\t* shrink\n",
    "\tmovements.\n",
    "\n",
    "<div class=\"center\" style=\"width: auto; margin-left: auto; margin-right: auto;\"> ![](assets/figs/optimization/neldermeadsteps.jpg) </div>\n",
    "\n",
    "* this is a very popular method. The matlab functions `fmincon` and `fminsearch` implements it.\n",
    "* When it works, it works quite fast.\n",
    "* No derivatives required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999710322210338,0.9999438685860869]\n",
       " * Minimum: 1.164323e-09\n",
       " * Iterations: 74\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock, [0.0, 0.0], NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* But.\n",
    "\n",
    "\n",
    "## Bracketing for Multidimensional Problems: Comment on Nelder-Mead\n",
    "\n",
    "> Lagarias et al. (SIOPT, 1999):\n",
    "At present there is no function in any dimension greater than one, for which the original Nelder-Mead algorithm has been proved to converge to a minimizer.\n",
    "\n",
    ">Given all the known inefficiencies and failures of the Nelder-Mead algorithm [...], one might wonder why it is used at all, let alone why it is so extraordinarily popular.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Reminder: Optimality Conditions \n",
    "\n",
    "### Notation\n",
    "\n",
    "\n",
    "* Unless otherwise noted, we have $x \\in \\mathbb{R}^n$ as an $n$ element vector.\n",
    "* The **gradient** of a function $f : \\mathbb{R}^n \\mapsto \\mathbb{R}$ is denoted $\\nabla f:\\mathbb{R}^n \\mapsto \\mathbb{R}^n$ a it returns a vector\n",
    "\t$$ \\nabla f(x) = \\left(\\frac{\\partial f}{\\partial x_1}(x),\\frac{\\partial f}{\\partial x_2}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x) \\right) $$\n",
    "* It's **hessian** is a function denoted $\\nabla^2 f(x)$ or $H_f :\\mathbb{R}^n \\mapsto \\mathbb{R}^{n\\times n}$ and returns an $(n,n)$ matrix given by\n",
    "\t$$  H_f(x) = \\left( \\begin{array}{c} \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1}(x)  &  \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(x) & \\dots & \\frac{\\partial^2 f}{\\partial x_n \\partial x_1}(x) \\\\\n",
    "\t                     \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(x)  &  \\frac{\\partial^2 f}{\\partial x_2 \\partial x_2}(x) & \\dots & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}(x) \\\\\n",
    "\t                     \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "\t                     \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}(x)  &  \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}(x) & \\dots & \\frac{\\partial^2 f}{\\partial x_n \\partial x_n}(x) \n",
    "\t                     \\end{array}\n",
    "\t             \\right)\n",
    "\t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Optimality Conditions\n",
    "\n",
    "* **First Order Necessary Conditions**:\n",
    "\tIf $f$ is continously differentiable and $x^*$ is a local minimizer of $f$, then $\\nabla f(x^*) = 0$.\n",
    "* **Second Order Necessary Conditions**:\n",
    "\tIf $f$ is twice continuously differentiable and $x^*$ is a local minimizer of $f$, then $\\nabla f(x^*) = 0$ *and* $H_f (x^*)$ is positive semi-definite, i.e. we have $s^T H_f (x^*) s \\geq 0$ for all $s \\in \\mathbb{R}^n$.\n",
    "* **Second Order Sufficient Conditions**:\n",
    "\tIf $f$ is twice continuously differentiable and at $x^*$ we have that $\\nabla f(x^*) = 0$ *and* $H_f (x^*)$ is positive definite, i.e. $s^T H_f (x^*) s > 0,s\\neq 0$ then $x^*$ is a local minimizer of $f$.\n",
    "* More sophisticated solvers make ample use of those.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: Sufficient Conditions for Global Optimality \n",
    "\n",
    "* If $f$ is convex, then any local minimizer $x^*$ is a global minimizer.\n",
    "*  If $f$ is convex and differentiable on $\\mathbb{R}^n$ , then any point $x^*$ is a global minimizer if and only if it is a stationary point, i.e. if f$\\nabla f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reminder: Taylor's Theorem\n",
    "\n",
    "* Many of the ensuing methods are based on Taylor's theorem, so let's remind ourselves of it:\n",
    "\n",
    "<!-- \tf(x) & = & f(x^0) + \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} (x^0) (x_i - x^0) \\\\\n",
    "\t     & + & \\frac{1}{2} + \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} (x^0) (x_i - x^0) \\\\\n",
    " -->\n",
    "\n",
    "* Suppose that $f \\in C^{n+1}[a,b]$, and $x,x^0 \\in [a,b]$. Then\n",
    "\t$$ \\begin{align}\n",
    "\tf(x)  = & f(x^0) + f'(x^0)(x - x^0) + f''(x^0)\\frac{(x- x^0)^2}{2}\\\\\n",
    "\t      + & \\dots + f^{(n)}(x^0)\\frac{(x - x^0)^n}{n!} + R_{n+1}(x) \n",
    "\t     \\end{align}\n",
    "\t     $$\n",
    "    where $R_{n+1}(x) = \\mathcal{o}\\left(\\Vert (x-x^0) \\Vert^{n+1}\\right)$ is reminder term that converges at a rate $n+1$ to zero, i.e. \n",
    "* we say a function $f$ is $\\mathcal{o}(\\Vert x \\Vert^n)$ if $\\lim_{x\\to 0}\\Vert f(x) \\Vert / \\Vert x \\Vert^n =0$.\n",
    "* we say a function $f$ is $\\mathcal{O}(\\Vert x \\Vert^n)$ if $\\lim_{x\\to 0}\\Vert f(x) \\Vert / \\Vert x \\Vert^n < \\infty$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A quick Note on Computing Derivatives\n",
    "\n",
    "* Finite Differences\n",
    "* Automatic Differentiation\n",
    "* We have talked about this in a separate session.\n",
    "* For now just remember that if we don't supply analytic gradients, and the algorithm requires them, this often triggers a numerical approximation of the gradient known as finite differences. This is most of the times a slow proceedure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Strategies: Line Search and Trust Region\n",
    "\n",
    "* We only provide an overview of methods here. If you want to *really* know the details about those algorithms, I invite you to consult [@nocedal-wright]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Line Search Strategy\n",
    "\n",
    "* An algorithm from the line search class  chooses a direction $p_k \\in \\mathbb{R}^n$ and searches along that direction starting from the current iterate $x_k \\in \\mathbb{R}^n$ for a new iterate $x_{k+1} \\in \\mathbb{R}^n$ with a lower function value.\n",
    "* After deciding on a direction $p_k$, one needs to decide the *step length* $\\alpha$ to travel by solving\n",
    "\t$$ \\min_{\\alpha>0} f(x_k + \\alpha p_k) $$\n",
    "* In practice, solving this exactly is too costly, so algos usually generate a sequence of trial values $\\alpha$ and pick the one with the lowest $f$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Trust Region Strategy\n",
    "\n",
    "* Here we construct a *model function* $m_k$ that is similar to $f$ around $x_k$.\n",
    "* We acknowledge that $m_k$ is decent approximation of $f$ only in some *region*.\n",
    "* The problem is then to find a candidate step length $p$ by solving\n",
    "\t$$ \\min_p m_k (x_k + p) $$ where $x_k + p$ lies inside the trust region.\n",
    "* If candidate $p$ does not produce a value lower than $f(x_k)$, we must have had a too large trust region, shrink it, and do it again.\n",
    "* Usually the trust region is a ball $\\Vert p \\Vert_2 \\leq \\Delta$, where $\\Delta$ is called the *trust region radius*, but elliptical and box regions are possible.\n",
    "* A common definition of the model function is a quadratic of the form\n",
    "\t$$ m_k(x_k + p) = f(x_k) + p^T \\nabla f(x_k) + \\frac{1}{2} p^T B(x_k) p $$\n",
    "\twhere gradient and matrix $B(x_k)$ are evaluated at the current iterate, so the model function is in agreement to first order with $f$ at the current guess.\n",
    "* The matrix $B(x_k)$ is either the Hessian $H_f$ or some approximation to it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Trust Region Example\n",
    "\n",
    "* Suppose we have $f(x) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$. At point $x_k= (0,1)$ gradient and hessian are:\n",
    "* ?\n",
    "\n",
    "$$ \\nabla f(x_k) = \\left[ \\begin{array}{c} -2 \\\\ 20 \\end{array} \\right], \\quad \\quad \\nabla^2 f(x_k) = \\left[ \\begin{array}{c} -38 & 0 \\\\ 0 & 20 \\end{array} \\right] $$\n",
    "\n",
    "![Figure 2.4 of [@nocedal-wright]](assets/figs-restricted/trust-region.png)\n",
    "\n",
    "* In this figure, we use $B(x_k) = \\nabla^2 f(x_k)$. \n",
    "* After each unsuccesful step, the new candidate step will be shorter, and will go in a different direction\n",
    "* This is the main difference to line search methods.\n",
    "* Main difference between the two methods: order in which they change *direction* and *step length*\n",
    "\t* line search fixes direction $p_k$ and finds right distance $\\alpha_k$\n",
    "\t* trust region fixes an appropriate radius $\\Delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Line Search Methods: Which Direction to go?\n",
    "\n",
    "\n",
    "### Steepest Descent\n",
    "\n",
    "* The direction $-\\nabla f(x_k)$ is an obvious choice: Among all possible directions, along this one $f$ decreases most rapidly.\n",
    "* This claim can be verified using Taylor's theorem.\n",
    "* The **steepest descent method** is a line search method that moves along\n",
    "\t$$ p_k = -\\nabla f(x_k) $$\n",
    "* The step length $\\alpha_k$ can be chosen in many different ways.\n",
    "* There are many other *descent* directions, steepest descent is but one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent = Results of Optimization Algorithm\n",
      " * Algorithm: Gradient Descent\n",
      " * Starting Point: [0.0,0.0]\n",
      " * Minimizer: [0.9356732500354086,0.875073922357589]\n",
      " * Minimum: 4.154782e-03\n",
      " * Iterations: 1000\n",
      " * Convergence: false\n",
      "   * |x - x'| < 1.0e-32: false\n",
      "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
      "   * |g(x)| < 1.0e-08: false\n",
      "   * f(x) > f(x'): false\n",
      "   * Reached Maximum Number of Iterations: true\n",
      " * Objective Function Calls: 3532\n",
      " * Gradient Calls: 3532\n",
      "\n",
      "\n",
      "gradient descent 2 = Results of Optimization Algorithm\n",
      " * Algorithm: Gradient Descent\n",
      " * Starting Point: [0.0,0.0]\n",
      " * Minimizer: [0.9978398797724763,0.9956717950747302]\n",
      " * Minimum: 4.682073e-06\n",
      " * Iterations: 5000\n",
      " * Convergence: false\n",
      "   * |x - x'| < 1.0e-32: false\n",
      "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
      "   * |g(x)| < 1.0e-08: false\n",
      "   * f(x) > f(x'): false\n",
      "   * Reached Maximum Number of Iterations: true\n",
      " * Objective Function Calls: 17532\n",
      " * Gradient Calls: 17532\n",
      "\n",
      "\n",
      "gradient descent 3 = Results of Optimization Algorithm\n",
      " * Algorithm: Gradient Descent\n",
      " * Starting Point: [0.0,0.0]\n",
      " * Minimizer: [0.9999999914304203,0.9999999828109042]\n",
      " * Minimum: 7.368706e-17\n",
      " * Iterations: 20458\n",
      " * Convergence: true\n",
      "   * |x - x'| < 1.0e-32: false\n",
      "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
      "   * |g(x)| < 1.0e-08: true\n",
      "   * f(x) > f(x'): false\n",
      "   * Reached Maximum Number of Iterations: false\n",
      " * Objective Function Calls: 71635\n",
      " * Gradient Calls: 71635\n"
     ]
    }
   ],
   "source": [
    "# there is a dedicated LineSeach package: https://github.com/JuliaNLSolvers/LineSearches.jl\n",
    "GD = optimize(rosenbrock.f, rosenbrock.g!,[0.0, 0.0],GradientDescent())\n",
    "GD1 = optimize(rosenbrock.f, rosenbrock.g!,[0.0, 0.0],GradientDescent(),Optim.Options(iterations=5000))\n",
    "GD2 = optimize(rosenbrock.f, rosenbrock.g!,[0.0, 0.0],GradientDescent(),Optim.Options(iterations=50000))\n",
    "\n",
    "println(\"gradient descent = $GD\")\n",
    "println(\"\\n\")\n",
    "println(\"gradient descent 2 = $GD1\")\n",
    "println(\"\\n\")\n",
    "println(\"gradient descent 3 = $GD2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 2.5 of [@nocedal-wright]](assets/figs-restricted/steepest-descent.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Line Search Methods: The **Newton** Direction\n",
    "\n",
    "* Probably the most important descent direction.\n",
    "* In vector notation, the 2nd order taylor series approximation to $f(x_k + p)$ is\n",
    "\t$$ f(x_k + p) \\approx f(x_k) + p^T \\nabla f(x_k) + \\frac{1}{2} p^T \\nabla^2 f(x_k) p \\equiv m_k(p) $$\n",
    "* the Newton direction is obtained by finding the vector $p$ that minimizes $m_k(p)$, i.e. by setting the derivative of $m_k(p)$ to zero.\n",
    "* We obtain\n",
    "\t$$ p_k^N = -(\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)  $$\n",
    "* The newton direction is reliable if the discrepancy between truth and model $m$ is not too large at $x_k$.\n",
    "* The biggest drawback is the need to compute the Hessian. This can be difficult analytically at times, and overly expensive numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0     1.000000e+00     2.000000e+00\n",
      "     1     8.431140e-01     1.588830e+00\n",
      "     2     6.776980e-01     3.453340e+00\n",
      "     3     4.954645e-01     4.862093e+00\n",
      "     4     3.041921e-01     2.590086e+00\n",
      "     5     1.991512e-01     3.780900e+00\n",
      "     6     9.531907e-02     1.299090e+00\n",
      "     7     5.657827e-02     2.445401e+00\n",
      "     8     2.257807e-02     1.839332e+00\n",
      "     9     6.626125e-03     1.314236e+00\n",
      "    10     8.689753e-04     5.438279e-01\n",
      "    11     4.951399e-06     7.814556e-02\n",
      "    12     9.065070e-10     6.017046e-04\n",
      "    13     9.337686e-18     1.059738e-07\n",
      "    14     3.081488e-31     1.110223e-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999999999994,0.9999999999999989]\n",
       " * Minimum: 3.081488e-31\n",
       " * Iterations: 14\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "   * |g(x)| < 1.0e-08: true\n",
       "   * f(x) > f(x'): false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 58\n",
       " * Gradient Calls: 58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(rosenbrock.f, rosenbrock.g!, rosenbrock.h!, [0.0, 0.0], Newton(),Optim.Options(show_trace=true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Quasi-Newton Methods\n",
    "\n",
    "* In response to the difficulties of getting the Hessian, quasi-newton methods propose to approximate $B(x_k)$ with something *similar* to the hessian.\n",
    "* Taylors Theorem implies that \n",
    "\t$$ \\nabla^2 f(x_{k+1} - x_{k}) \\approx \\nabla f(x_{k+1}) - \\nabla f(x_k) $$\n",
    "\tand so we choose a $B$ matrix that mimics this property.\n",
    "* This leads to the *secant condition*\n",
    "\t$$ B_{k+1} (x_{k+1} - x_{k}) = f(x_{k+1}) - \\nabla f(x_k) $$\n",
    "* There are different ways to update the hessian in this way.\n",
    "* One of the best known is the BFGS method (after Broydon, Fletcher, Goldfarb and Shanno).\n",
    "* Those methods get the search direction by using $B_k$ instead of the exact Hessian, i.e.\n",
    "\t$$ p_k = -B_k^{-1} \\nabla f(x_k)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize(rosenbrock.f,rosenbrock.g!,rosenbrock.h!,[0.0,0.0],BFGS()) = Results of Optimization Algorithm\n",
      " * Algorithm: BFGS\n",
      " * Starting Point: [0.0,0.0]\n",
      " * Minimizer: [0.9999999999373613,0.9999999998686219]\n",
      " * Minimum: 7.645563e-21\n",
      " * Iterations: 16\n",
      " * Convergence: true\n",
      "   * |x - x'| < 1.0e-32: false\n",
      "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
      "   * |g(x)| < 1.0e-08: true\n",
      "   * f(x) > f(x'): false\n",
      "   * Reached Maximum Number of Iterations: false\n",
      " * Objective Function Calls: 69\n",
      " * Gradient Calls: 69\n",
      "optimize(rosenbrock.f,rosenbrock.g!,rosenbrock.h!,[0.0,0.0],LBFGS()) = Results of Optimization Algorithm\n",
      " * Algorithm: L-BFGS\n",
      " * Starting Point: [0.0,0.0]\n",
      " * Minimizer: [1.0000000000000007,1.000000000000001]\n",
      " * Minimum: 5.374115e-30\n",
      " * Iterations: 21\n",
      " * Convergence: true\n",
      "   * |x - x'| < 1.0e-32: false\n",
      "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
      "   * |g(x)| < 1.0e-08: true\n",
      "   * f(x) > f(x'): false\n",
      "   * Reached Maximum Number of Iterations: false\n",
      " * Objective Function Calls: 90\n",
      " * Gradient Calls: 90\n"
     ]
    }
   ],
   "source": [
    "@show optimize(rosenbrock.f, rosenbrock.g!, rosenbrock.h!,  [0.0, 0.0], BFGS());\n",
    "# low memory BFGS\n",
    "@show optimize(rosenbrock.f, rosenbrock.g!, rosenbrock.h!,  [0.0, 0.0], LBFGS());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Practical Considerations\n",
    "\n",
    "### Stopping criteria\n",
    "\n",
    "* In all of the above examples, we did not alter the default values for stopping criteria.\n",
    "* There are different things you could focus on as a stopping criterion with `Optim`, and similarly in most solver packages.\n",
    "\t* `xtol`: changes in $x$ from one iterate to the next\n",
    "\t* `ftol`: perentage changes in $f$ from one iterate to the next\n",
    "\t* `grtol`: absolute value of the gradient beign smaller than this number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Some Applications in Economics\n",
    "\n",
    "* Maximum Likelihood Estimation\n",
    "* The Nested Fixed Point Algorithm\n",
    "\t* [@rust-bus] is a maximum likelihood estimation with an inner loop that solves a dynamic programming problem.\n",
    "\t* [@BLP] is a GMM estimation with an inner fixed point problem."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
